---
# In Reddit, each participant is a Reddit author

# *Default configuration to simplify experiment setup
defaults:
  - base # Inherit from base config
  - _self_ # This ensures your config overrides the base config
  - atk_config: cifar10_multishot

# *Aggregator
aggregator: unweighted_fedavg

# *Specify if the attack is enabled
no_attack: False

# *Simulation configuration
num_rounds: 600
num_clients: 4000 # To be determined by at runtime (each user is a client)
num_clients_per_round: 50
federated_evaluation: False # Whether to perform federated evaluation (evaluate global model on client's validation set)
federated_val_split: 0.0 # Validation split for each client's dataset (should be > 0 if federated evaluation is True)

# *Dataset distribution configuration
dataset: sentiment140
datapath: data
prefetch_tokenizer: False
task: classification
partitioner: uniform
normalize: False

# *Model configuration
model: albert  # or "albert" when using that model
num_classes: 2  # Binary classification (positive/negative)
max_length: 128 # Maximum sequence length for tokenization

# *Test configuration
test_batch_size: 128
num_workers: 4 # Number of workers for dataloader
pin_memory: False
test_every: 1 # Test every x rounds

# *Default local training configuration (passed to clients during training)
client_config:
  # Training configuration
  task: ${task}
  local_epochs: 1
  batch_size: 32
  optimizer: adamw # Choose from the key in optimizer section below. Will be overridden to store DictConfig in client_optimizer_config
  lr: 2e-5
  weight_decay: 0.01
